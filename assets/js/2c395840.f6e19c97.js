"use strict";(globalThis.webpackChunknlp_essentials_textbook=globalThis.webpackChunknlp_essentials_textbook||[]).push([[8709],{1515(e,s,i){i.r(s),i.d(s,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"chapters/distributional_semantics/homework","title":"Homework","description":"HW4: Distributional Semantics","source":"@site/docs/chapters/distributional_semantics/homework.md","sourceDirName":"chapters/distributional_semantics","slug":"/chapters/distributional_semantics/homework","permalink":"/nlp-essentials/chapters/distributional_semantics/homework","draft":false,"unlisted":false,"editUrl":"https://github.com/emory-courses/nlp-essentials/tree/main/docs/chapters/distributional_semantics/homework.md","tags":[],"version":"current","frontMatter":{"title":"Homework","description":"HW4: Distributional Semantics"},"sidebar":"chaptersSidebar","previous":{"title":"Neural Language Models","permalink":"/nlp-essentials/chapters/distributional_semantics/neural-language-models"},"next":{"title":"Overview","permalink":"/nlp-essentials/chapters/large_language_models/overview"}}');var n=i(4848),r=i(8453);const o={title:"Homework",description:"HW4: Distributional Semantics"},a="Homework",l={},d=[{value:"Task 1",id:"task-1",level:2},{value:"Task 2",id:"task-2",level:2},{value:"Task 3",id:"task-3",level:2},{value:"Submission <a></a>",id:"submission-",level:2},{value:"Rubric",id:"rubric",level:2}];function c(e){const s={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(s.header,{children:(0,n.jsx)(s.h1,{id:"homework",children:"Homework"})}),"\n",(0,n.jsxs)(s.p,{children:["Create a ",(0,n.jsx)(s.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/src/homework/distributional_semantics.py",children:(0,n.jsx)(s.strong,{children:"distributional_semantics.py"})})," file in the ",(0,n.jsx)(s.a,{href:"https://github.com/emory-courses/nlp-essentials/tree/main/src/homework",children:"src/homework/"})," directory."]}),"\n",(0,n.jsx)(s.h2,{id:"task-1",children:"Task 1"}),"\n",(0,n.jsxs)(s.p,{children:["Your task is to read word embeddings trained by ",(0,n.jsx)(s.a,{href:"/nlp-essentials/chapters/distributional_semantics/neural-language-models",children:"Word2Vec"}),":"]}),"\n",(0,n.jsxs)(s.ol,{children:["\n",(0,n.jsxs)(s.li,{children:["Define a function called ",(0,n.jsx)(s.code,{children:"read_word_embeddings()"})," that takes a path to the file consisting of word embeddings, ",(0,n.jsx)(s.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/dat/word_embeddings.txt",children:"word_embeddings.txt"}),"."]}),"\n",(0,n.jsxs)(s.li,{children:["Return a dictionary where the key is a word and the value is its corresponding embedding in ",(0,n.jsx)(s.a,{href:"https://numpy.org/doc/stable/reference/generated/numpy.array.html",children:"numpy.array"}),"."]}),"\n"]}),"\n",(0,n.jsx)(s.p,{children:"Each line in the file adheres to the following format:"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{children:"[WORD](\\t[FLOAT]){50}\n"})}),"\n",(0,n.jsx)(s.h2,{id:"task-2",children:"Task 2"}),"\n",(0,n.jsx)(s.p,{children:"Your task is to retrieve a list of the most similar words to a given target word:"}),"\n",(0,n.jsxs)(s.ol,{children:["\n",(0,n.jsxs)(s.li,{children:["Define a function called ",(0,n.jsx)(s.code,{children:"similar_words()"})," that takes the word embeddings from Task 1, a target word (string), and a threshold (float)."]}),"\n",(0,n.jsx)(s.li,{children:"Return a list of tuples, where each tuple contains a word similar to the target word and the cosine similarity between them as determined by the embeddings. The returned list must only include words with similarity scores greater than or equal to the threshold, sorted in descending order based on the similarity scores."}),"\n"]}),"\n",(0,n.jsx)(s.h2,{id:"task-3",children:"Task 3"}),"\n",(0,n.jsx)(s.p,{children:"Your task is to measure a similarity score between two documents:"}),"\n",(0,n.jsxs)(s.ol,{children:["\n",(0,n.jsxs)(s.li,{children:["Define a function called ",(0,n.jsx)(s.code,{children:"document_similarity()"})," that takes the word embeddings and two documents (string). Assume that the documents are already tokenized."]}),"\n",(0,n.jsx)(s.li,{children:"For each document, generate a document embedding by averaging the embeddings of all words within the document."}),"\n",(0,n.jsx)(s.li,{children:"Return the cosine similarity between the two document embeddings."}),"\n"]}),"\n",(0,n.jsxs)(s.h2,{id:"submission-",children:["Submission ",(0,n.jsx)("a",{href:"#submission",id:"submission"})]}),"\n",(0,n.jsxs)(s.p,{children:["Commit and push the ",(0,n.jsx)(s.strong,{children:"distributional_semantics.py"})," file to your GitHub repository."]}),"\n",(0,n.jsx)(s.h2,{id:"rubric",children:"Rubric"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:"Task 1: Read Word Embeddings (2.8 points)"}),"\n",(0,n.jsx)(s.li,{children:"Task 2: Similar Words (3 points)"}),"\n",(0,n.jsx)(s.li,{children:"Task 3: Document Similarity (3 points)"}),"\n",(0,n.jsx)(s.li,{children:"Concept Quiz (3.2 points)"}),"\n"]})]})}function h(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,n.jsx)(s,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},8453(e,s,i){i.d(s,{R:()=>o,x:()=>a});var t=i(6540);const n={},r=t.createContext(n);function o(e){const s=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function a(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),t.createElement(r.Provider,{value:s},e.children)}}}]);