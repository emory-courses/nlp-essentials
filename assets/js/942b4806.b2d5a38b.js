"use strict";(globalThis.webpackChunknlp_essentials_textbook=globalThis.webpackChunknlp_essentials_textbook||[]).push([[1702],{2412(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"chapters/vector_space_models/document-classification","title":"Document Classification","description":"Document classification, also known as text classification, is a task that involves assigning predefined categories or labels to documents based on their content, used to automatically organize, categorize, or label large collections of textual documents.","source":"@site/docs/chapters/vector_space_models/document-classification.md","sourceDirName":"chapters/vector_space_models","slug":"/chapters/vector_space_models/document-classification","permalink":"/nlp-essentials/chapters/vector_space_models/document-classification","draft":false,"unlisted":false,"editUrl":"https://github.com/emory-courses/nlp-essentials/tree/main/docs/chapters/vector_space_models/document-classification.md","tags":[],"version":"current","frontMatter":{"title":"Document Classification"},"sidebar":"chaptersSidebar","previous":{"title":"Document Similarity","permalink":"/nlp-essentials/chapters/vector_space_models/document-similarity"},"next":{"title":"Homework","permalink":"/nlp-essentials/chapters/vector_space_models/homework"}}');var a=t(4848),i=t(8453);const o={title:"Document Classification"},r="Document Classification",c={},l=[{value:"Supervised Learning",id:"supervised-learning",level:2},{value:"Data Split",id:"data-split",level:2},{value:"Vectorization",id:"vectorization",level:2},{value:"Classification",id:"classification",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",admonition:"admonition",annotation:"annotation",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mi:"mi",mrow:"mrow",ol:"ol",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"document-classification",children:"Document Classification"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Document classification"}),", also known as text classification, is a task that involves assigning predefined categories or labels to documents based on their content, used to automatically organize, categorize, or label large collections of textual documents."]}),"\n",(0,a.jsx)(n.h2,{id:"supervised-learning",children:"Supervised Learning"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Supervised learning"})," is a machine learning paradigm where the algorithm is trained on a labeled dataset, with each data point (instance) being associated with a corresponding target label or output. The goal of supervised learning is to learn a mapping function from input features to output labels, which enables the algorithm to make predictions or decisions on unseen data."]}),"\n",(0,a.jsx)(n.h2,{id:"data-split",children:"Data Split"}),"\n",(0,a.jsxs)(n.p,{children:["Supervised learning typically involves dividing the entire dataset into training, development, and evaluation sets. The ",(0,a.jsx)(n.strong,{children:"training set"})," is used to train a model, the ",(0,a.jsx)(n.strong,{children:"development set"})," to tune the model's hyperparameters, and the ",(0,a.jsx)(n.strong,{children:"evaluation set"})," to assess the best model tuned on the development set."]}),"\n",(0,a.jsx)(n.p,{children:"It is critical to ensure that the evaluation set is never used to tune the model during training. Common practice involves splitting the dataset such as 80/10/10 or 75/10/15 for training, development, and evaluation sets, respectively."}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/tree/main/dat/document_classification",children:"document_classification"})," directory contains the training (trn), development (dev), and evaluation (tst) sets comprising 82, 14, and 14 documents, respectively. Each document is a chapter from the ",(0,a.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/dat/chronicles_of_narnia.txt",children:"chronicles_of_narnia.txt"})," file, following a file-naming convention of ",(0,a.jsx)(n.code,{children:"A_B"}),", where ",(0,a.jsx)(n.code,{children:"A"})," denotes the book ID and ",(0,a.jsx)(n.code,{children:"B"})," indicates the chapter ID."]}),"\n",(0,a.jsx)(n.p,{children:"Let us define a function that takes a path to a directory containing training documents and returns a dictionary, where each key in the dictionary corresponds to a book label, and its associated value is a list of documents within that book:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:"from src.bag_of_words_model import Document\nimport glob, os\n\ndef collect(dirpath: str) -> dict[int, list[Document]]:\n    books = dict()\n\n    for filename in glob.glob(os.path.join(dirpath, '*.txt')):\n        t = os.path.basename(filename).split('_')\n        book_id = int(t[0])\n        fin = open((filename))\n        books.setdefault(book_id, list()).append(fin.read().split())\n\n    return books\n"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["L7: the ",(0,a.jsx)(n.a,{href:"https://docs.python.org/3/library/glob.html",children:"glob"})," module, the ",(0,a.jsx)(n.a,{href:"https://docs.python.org/3/library/os.html",children:"os"})," module."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"We then print the number of documents in each set:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:'showLineNumbers title="Run"',children:"def join_documents(dataset: dict[int, list[Document]]) -> list[Document]:\n    return [document for documents in dataset.values() for document in documents]\n\ntrn = collect('dat/document_classification/trn')\ndev = collect('dat/document_classification/dev')\ntst = collect('dat/document_classification/tst')\nprint(len(join_documents(trn)), len(join_documents(dev)), len(join_documents(tst)))\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:'title="Output"',children:"82 14 14\n"})}),"\n",(0,a.jsx)(n.admonition,{type:"warning",children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Q8"}),": What potential problems might arise from the above ",(0,a.jsx)(n.strong,{children:"data splitting"})," approach, and what alternative method could mitigate these issues?"]})}),"\n",(0,a.jsx)(n.h2,{id:"vectorization",children:"Vectorization"}),"\n",(0,a.jsx)(n.p,{children:"To vectorize the documents, let us gather the vocabulary and their document frequencies from the training set:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:"corpus = join_documents(trn)\nvocab = vocabulary(join_documents(trn))\ndfs = document_frequencies(vocab, corpus)\nD = len(corpus)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Let us create a function that takes the vocabulary, document frequencies, document length, and a document set, and returns a list of tuples, where each tuple consists of a book ID and a sparse vector representing a document in the corresponding book:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:"def vectorize(vocab: Vocab, dfs: SparseVector, D: int, docset: dict[int, list[Document]]) -> list[tuple[int, SparseVector]]:\n    vs = []\n\n    for book_id, documents in docset.items():\n        for document in documents:\n            vs.append((book_id, tf_idf(vocab, dfs, D, document)))\n\n    return vs\n"})}),"\n",(0,a.jsx)(n.p,{children:"We then vectorize all documents in each set:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:"trn_vs = vectorize(vocab, dfs, D, trn)\ndev_vs = vectorize(vocab, dfs, D, dev)\ntst_vs = vectorize(vocab, dfs, D, tst)\n"})}),"\n",(0,a.jsx)(n.admonition,{type:"warning",children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Q9"}),": Why do we use only the ",(0,a.jsx)(n.strong,{children:"training set"})," to collect the vocabulary?"]})}),"\n",(0,a.jsx)(n.h2,{id:"classification",children:"Classification"}),"\n",(0,a.jsxs)(n.p,{children:["Let us develop a classification model using the ",(0,a.jsx)(n.strong,{children:"K-nearest neighbors algorithm"})," [1] that takes the training vector set, a document, and ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"k"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"k"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6944em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.03148em"},children:"k"})]})})]}),", and returns the predicted book ID of the document and its similarity score:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:"def knn(trn_vs: list[tuple[int, SparseVector]], v: SparseVector, k: int = 1) -> tuple[int, float]:\n    sims = [(book_id, cosine_similarity(v, t)) for book_id, t in trn_vs]\n    sims.sort(key=lambda x: x[1], reverse=True)\n    return Counter(sims[:k]).most_common(1)[0][0]\n"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["L2: Measure the similarity between the input document ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"v"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"v"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.4306em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"v"})]})})]})," and every document ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"t"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"t"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6151em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"t"})]})})]})," in the training set and save it with the book ID of ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"t"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"t"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6151em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"t"})]})})]}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["L3-4: Return the most common book ID among the top-",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"k"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"k"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6944em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.03148em"},children:"k"})]})})]})," documents in the training set that are most similar to ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"v"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"v"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.4306em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"v"})]})})]}),"."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Finally, we test our classification model on the development set:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:'showLineNumbers title="Run"',children:"correct = 0\n\nfor g_book_id, document in dev_vs:\n    p_book_id, p_score = knn(trn_vs, document)\n    if g_book_id == p_book_id: correct += 1\n    print('Gold: {}, Auto: {}, Score: {:.2f}'.format(g_book_id, p_book_id, p_score))\n\nprint('Accuracy: {} ({}/{})'.format(100 * correct / len(dev_vs), correct, len(dev_vs)))\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",metastring:'title="Output"',children:"Gold: 1, Auto: 1, Score: 0.49\nGold: 1, Auto: 1, Score: 0.27\nGold: 3, Auto: 3, Score: 0.36\nGold: 3, Auto: 3, Score: 0.32\nGold: 5, Auto: 5, Score: 0.29\nGold: 5, Auto: 5, Score: 0.54\nGold: 0, Auto: 0, Score: 0.32\nGold: 0, Auto: 0, Score: 0.26\nGold: 6, Auto: 6, Score: 0.48\nGold: 6, Auto: 6, Score: 0.49\nGold: 2, Auto: 2, Score: 0.37\nGold: 2, Auto: 2, Score: 0.31\nGold: 4, Auto: 4, Score: 0.56\nGold: 4, Auto: 4, Score: 0.60\nAccuracy: 100.0 (14/14)\n"})}),"\n",(0,a.jsx)(n.admonition,{type:"warning",children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Q10"}),": What are the primary weaknesses and limitations of the ",(0,a.jsx)(n.strong,{children:"K-Nearest Neighbors (KNN)"})," classification model when applied to document classification?"]})}),"\n",(0,a.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,a.jsxs)(n.p,{children:["Source: ",(0,a.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/src/document_classification.py",children:"document_classification.py"})]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm",children:"K-nearest neighbors"}),", Wikipedia."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>r});var s=t(6540);const a={},i=s.createContext(a);function o(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);