"use strict";(globalThis.webpackChunknlp_essentials_textbook=globalThis.webpackChunknlp_essentials_textbook||[]).push([[671],{4112(e,n,t){t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"chapters/text_processing/tokenization","title":"Tokenization","description":"Tokenization is the process of breaking down a text into smaller units, typically words or subwords, known as tokens. Tokens serve as the basic building blocks used for a specific task.","source":"@site/docs/chapters/text_processing/tokenization.md","sourceDirName":"chapters/text_processing","slug":"/chapters/text_processing/tokenization","permalink":"/nlp-essentials/chapters/text_processing/tokenization","draft":false,"unlisted":false,"editUrl":"https://github.com/emory-courses/nlp-essentials/tree/main/docs/chapters/text_processing/tokenization.md","tags":[],"version":"current","frontMatter":{"title":"Tokenization"},"sidebar":"chaptersSidebar","previous":{"title":"Frequency Analysis","permalink":"/nlp-essentials/chapters/text_processing/frequency-analysis"},"next":{"title":"Lemmatization","permalink":"/nlp-essentials/chapters/text_processing/lemmatization"}}');var i=t(4848),r=t(8453);const o={title:"Tokenization"},l="Tokenization",a={},d=[{value:"Delimiters",id:"delimiters",level:2},{value:"Post-Processing",id:"post-processing",level:2},{value:"Tokenizing",id:"tokenizing",level:2},{value:"References",id:"references",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"tokenization",children:"Tokenization"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tokenization"})," is the process of breaking down a text into smaller units, typically words or subwords, known as tokens. ",(0,i.jsx)(n.strong,{children:"Tokens"})," serve as the basic building blocks used for a specific task."]}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Q3"}),": What is the difference between a ",(0,i.jsx)(n.strong,{children:"word"})," and a ",(0,i.jsx)(n.strong,{children:"token"}),"?"]})}),"\n",(0,i.jsxs)(n.p,{children:["When examining the ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/tree/main/dat/word_types.txt",children:"dat/word_types.txt"})," from the previous section, you notice several words that need further tokenization, where many of them can be resolved by leveraging punctuation:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:['"R1: \u2192 ',(0,i.jsx)(n.code,{children:'[\'"\', "R1", ":"]'})]}),"\n",(0,i.jsxs)(n.li,{children:["(R&D) \u2192 ",(0,i.jsx)(n.code,{children:"['(', 'R&D', ')']"})]}),"\n",(0,i.jsxs)(n.li,{children:["15th-largest \u2192 ",(0,i.jsx)(n.code,{children:"['15th', '-', 'largest']"})]}),"\n",(0,i.jsxs)(n.li,{children:["Atlanta, \u2192 ",(0,i.jsx)(n.code,{children:"['Atlanta', ',']"})]}),"\n",(0,i.jsxs)(n.li,{children:["Department's \u2192 ",(0,i.jsx)(n.code,{children:"['Department', \"'s\"]"})]}),"\n",(0,i.jsxs)(n.li,{children:['activity"[26] \u2192 ',(0,i.jsx)(n.code,{children:"['activity', '\"', '[26]']"})]}),"\n",(0,i.jsxs)(n.li,{children:["centers.[21][22] \u2192 ",(0,i.jsx)(n.code,{children:"['centers', '.', '[21]', '[22]']"})]}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:['\u261d\ufe0f Depending on the task, you may want to tokenize "[26]" into ',(0,i.jsx)(n.code,{children:"['[', '26', ']']"}),' for more generalization. In this case, however, we consider "[26]" as a unique identifier for the corresponding reference rather than as the number "26" surrounded by square brackets. Thus, we aim to recognize it as a single token.']})}),"\n",(0,i.jsx)(n.h2,{id:"delimiters",children:"Delimiters"}),"\n",(0,i.jsxs)(n.p,{children:["Let us write the ",(0,i.jsx)(n.code,{children:"delimit()"})," function that takes a word and a set of delimiters, and returns a list of tokens by splitting the word using the delimiters:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:"def delimit(word: str, delimiters: set[str]) -> list[str]:\n    i = next((i for i, c in enumerate(word) if c in delimiters), -1)\n    if i < 0: return [word]\n    tokens = []\n\n    if i > 0: tokens.append(word[:i])\n    tokens.append(word[i])\n\n    if i + 1 < len(word):\n        tokens.extend(delimit(word[i + 1:], delimiters))\n\n    return tokens\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L1: ",(0,i.jsx)(n.a,{href:"https://docs.python.org/3.12/library/typing.html",children:"Support for type hints"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["L2: Find the index of the first character in ",(0,i.jsx)(n.code,{children:"word"})," that is in a ",(0,i.jsx)(n.a,{href:"https://docs.python.org/3/library/stdtypes.html#set",children:"set"})," of ",(0,i.jsx)(n.code,{children:"delimiters"})," (",(0,i.jsx)(n.a,{href:"https://docs.python.org/3/library/functions.html#enumerate",children:"enumerate()"}),", ",(0,i.jsx)(n.a,{href:"https://docs.python.org/3/library/functions.html#next",children:"next()"}),"). If no delimiter is found in ",(0,i.jsx)(n.code,{children:"word"}),", return -1 (",(0,i.jsx)(n.a,{href:"https://docs.python.org/3/reference/expressions.html#generator-expressions",children:"generator expressions"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:["L3: If no delimiter is found, return a list containing ",(0,i.jsx)(n.code,{children:"word"})," as a single token."]}),"\n",(0,i.jsxs)(n.li,{children:["L4: If a delimiter is found, create a list ",(0,i.jsx)(n.code,{children:"tokens"})," to store the individual tokens."]}),"\n",(0,i.jsxs)(n.li,{children:["L6: If the delimiter is not at the beginning of ",(0,i.jsx)(n.code,{children:"word"}),", add the characters before the delimiter as a token to ",(0,i.jsx)(n.code,{children:"tokens"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["L7: Add the delimiter itself as a separate token to ",(0,i.jsx)(n.code,{children:"tokens"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["L9-10: If there are characters after the delimiter, call ",(0,i.jsx)(n.code,{children:"delimit()"})," recursively on the remaining part of ",(0,i.jsx)(n.code,{children:"word"})," and ",(0,i.jsx)(n.a,{href:"https://docs.python.org/3/tutorial/datastructures.html#more-on-lists",children:"extend()"})," the ",(0,i.jsx)(n.code,{children:"tokens"})," list with the result."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Let us define a set of delimiters and test ",(0,i.jsx)(n.code,{children:"delimit()"})," using various input:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'showLineNumbers title="Run"',children:"delims = {'\"', \"'\", '(', ')', '[', ']', ':', '-', ',', '.'}\n\ninput = [\n    '\"R1:',\n    '(R&D)',\n    '15th-largest',\n    'Atlanta,',\n    \"Department's\",\n    'activity\"[26]',\n    'centers.[21][22]',\n    '149,000',\n    'U.S.'\n]\n\noutput = [delimit(word, delims) for word in input]\n\nfor word, tokens in zip(input, output):\n    print('{:<16} -> {}'.format(word, tokens))\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L1: ",(0,i.jsx)(n.a,{href:"https://docs.python.org/3/library/stdtypes.html?highlight=list#set-types-set-frozenset",children:"Set types"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["L17: Iterate over the two lists, ",(0,i.jsx)(n.code,{children:"input"})," and ",(0,i.jsx)(n.code,{children:"output"}),", in parallel using the ",(0,i.jsx)(n.a,{href:"https://docs.python.org/3/library/functions.html#zip",children:"zip()"})," function."]}),"\n",(0,i.jsxs)(n.li,{children:["L18: ",(0,i.jsx)(n.a,{href:"https://docs.python.org/3/library/string.html#format-specification-mini-language",children:"Format Specification Mini-Language"})]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",metastring:'title="Output"',children:"\"R1: -> ['\"', 'R1', ':']\n(R&D) -> ['(', 'R&D', ')']\n15th-largest -> ['15th', '-', 'largest']\nAtlanta, -> ['Atlanta', ',']\nDepartment's -> ['Department', \"'\", 's']\nactivity\"[26] -> ['activity', '\"', '[', '26', ']']\ncenters.[21][22] -> ['centers', '.', '[', '21', ']', '[', '22', ']']\n149,000 -> ['149', ',', '000']\nU.S. -> ['U', '.', 'S', '.']\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Q4"}),": All delimiters used in our implementation are ",(0,i.jsx)(n.strong,{children:"punctuation marks"}),". What types of tokens should not be split by such delimiters?"]})}),"\n",(0,i.jsx)(n.h2,{id:"post-processing",children:"Post-Processing"}),"\n",(0,i.jsxs)(n.p,{children:["When reviewing the output of ",(0,i.jsx)(n.code,{children:"delimit()"}),", the first four test cases yield accurate results, while the last five are not handled properly, which should have been tokenized as follows:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Department's \u2192 ",(0,i.jsx)(n.code,{children:"['Department', \"'s\"]"})]}),"\n",(0,i.jsxs)(n.li,{children:['activity"[26] \u2192 ',(0,i.jsx)(n.code,{children:"['activity', '\"', '[26]']"})]}),"\n",(0,i.jsxs)(n.li,{children:["centers.[21][22] \u2192 ",(0,i.jsx)(n.code,{children:"['centers', '.', '[21]', '[22]']"})]}),"\n",(0,i.jsxs)(n.li,{children:["149,000 \u2192 ",(0,i.jsx)(n.code,{children:"['149,000']"})]}),"\n",(0,i.jsxs)(n.li,{children:["U.S. \u2192 ",(0,i.jsx)(n.code,{children:"['U.S.']"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["To handle these special cases, let us post-process the tokens generated by ",(0,i.jsx)(n.code,{children:"delimit()"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:"def postprocess(tokens: list[str]) -> list[str]:\n    i, new_tokens = 0, []\n\n    while i < len(tokens):\n        if i + 1 < len(tokens) and tokens[i] == \"'\" and tokens[i + 1].lower() == 's':\n            new_tokens.append(''.join(tokens[i:i + 2]))\n            i += 1\n        elif i + 2 < len(tokens) and \\\n                ((tokens[i] == '[' and tokens[i + 1].isnumeric() and tokens[i + 2] == ']') or\n                 (tokens[i].isnumeric() and tokens[i + 1] == ',' and tokens[i + 2].isnumeric())):\n            new_tokens.append(''.join(tokens[i:i + 3]))\n            i += 2\n        elif i + 3 < len(tokens) and ''.join(tokens[i:i + 4]) == 'U.S.':\n            new_tokens.append(''.join(tokens[i:i + 4]))\n            i += 3\n        else:\n            new_tokens.append(tokens[i])\n        i += 1\n\n    return new_tokens\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L2: Initialize variables ",(0,i.jsx)(n.code,{children:"i"})," for the current position and ",(0,i.jsx)(n.code,{children:"new_tokens"})," for the resulting tokens."]}),"\n",(0,i.jsx)(n.li,{children:"L4: Iterate through the input tokens."}),"\n",(0,i.jsxs)(n.li,{children:['L5: Case 1: Handling apostrophes for contractions like "',(0,i.jsx)(n.em,{children:"'s"}),'" (e.g., ',(0,i.jsx)(n.em,{children:"it's"}),").","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:['L6: Combine the apostrophe and "',(0,i.jsx)(n.em,{children:"s"}),'" and append it as a single token.']}),"\n",(0,i.jsx)(n.li,{children:"L7: Move the position indicator by 1 to skip the next character."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["L8-10: Case 2: Handling numbers in special formats like [##], ###,### (e.g., [42], 12,345).","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"L11: Combine the special number format and append it as a single token."}),"\n",(0,i.jsx)(n.li,{children:"L12: Move the position indicator by 2 to skip the next two characters."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:['L13: Case 3: Handling acronyms like "',(0,i.jsx)(n.em,{children:"U.S."}),'".',"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"L14: Combine the acronym and append it as a single token."}),"\n",(0,i.jsx)(n.li,{children:"L15: Move the position indicator by 3 to skip the next three characters."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"L17: Case 4: If none of the special cases above are met, append the current token."}),"\n",(0,i.jsx)(n.li,{children:"L18: Move the position indicator by 1 to process the next token."}),"\n",(0,i.jsx)(n.li,{children:"L20: Return the list of processed tokens."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Once the post-processing is applied, all outputs are now handled properly:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'showLineNumbers title="Run"',children:"output = [postprocess(delimit(word, delims)) for word in input]\n\nfor word, tokens in zip(input, output):\n    print('{:<16} -> {}'.format(word, tokens))\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",metastring:'title="Output"',children:"\"R1: -> ['\"', 'R1', ':']\n(R&D) -> ['(', 'R&D', ')']\n15th-largest -> ['15th', '-', 'largest']\nAtlanta, -> ['Atlanta', ',']\nDepartment's -> ['Department', \"'s\"]\nactivity\"[26] -> ['activity', '\"', '[26]']\ncenters.[21][22] -> ['centers', '.', '[21]', '[22]']\n149,000 -> ['149,000']\nU.S. -> ['U.S.']\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Q5"}),": Our tokenizer uses ",(0,i.jsx)(n.strong,{children:"hard-coded rules"})," to handle specific cases. What would be a ",(0,i.jsx)(n.strong,{children:"scalable"})," approach to handling more diverse cases?"]})}),"\n",(0,i.jsx)(n.h2,{id:"tokenizing",children:"Tokenizing"}),"\n",(0,i.jsxs)(n.p,{children:["Finally, let us write ",(0,i.jsx)(n.code,{children:"tokenize()"})," that takes a path to a corpus and a set of delimiters, and returns a list of tokens from the corpus:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:"def tokenize(corpus: str, delimiters: set[str]) -> list[str]:\n    with open(corpus) as fin:\n        words = fin.read().split()\n    return [token for word in words for token in postprocess(delimit(word, delimiters))]\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L2: Read the ",(0,i.jsx)(n.code,{children:"corpus"})," file."]}),"\n",(0,i.jsx)(n.li,{children:"L3: Split the text into words."}),"\n",(0,i.jsxs)(n.li,{children:["L4: Tokenize each word in the corpus using the specified delimiters. ",(0,i.jsx)(n.code,{children:"postprocess()"})," is used to process the special cases further. The resulting tokens are collected in a list and returned (",(0,i.jsx)(n.a,{href:"https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions",children:"list comprehension"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Given the new tokenizer, let us recount word types in the corpus, ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/dat/text_processing/emory-wiki.txt",children:"emory-wiki.txt"}),", and save them:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'showLineNumbers title="Run"',children:"from collections import Counter\nfrom src.frequency_analysis import save_output\n\ncorpus = 'dat/emory-wiki.txt'\noutput = 'dat/word_types-token.txt'\n\nwords = tokenize(corpus, delims)\ncounts = Counter(words)\n\nprint(f'# of word tokens: {len(words)}')\nprint(f'# of word types: {len(counts)}')\n\nsave_output(counts, output)\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L2: Import ",(0,i.jsx)(n.code,{children:"save_output()"})," from the ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/src/frequency_analysis.py",children:"src/frequency_analysis.py"})," module."]}),"\n",(0,i.jsxs)(n.li,{children:["L13: Save the tokenized output to ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/dat/word_types-token.txt",children:"dat/word_types-token.txt"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",metastring:'title="Output"',children:"# of word tokens: 363\n# of word types: 197\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/dat/text_processing/word_types-token.txt",children:"dat/text_processing /word_types-token.txt"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Compared to the original tokenization, where all words are split solely by whitespaces, the more advanced tokenizer increases the number of word tokens from 305 to 363 and the number of word types from 180 to 197 because all punctuation symbols, as well as reference numbers, are now introduced as individual tokens."}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Q6"}),": The use of a more advanced tokenizer mitigates the issue of sparsity. What exactly is the ",(0,i.jsx)(n.strong,{children:"sparsity issue"}),", and how can appropriate tokenization help alleviate it?"]})}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Source: ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/src/tokenization.py",children:"src/tokenization.py"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/emorynlp/elit-tokenizer",children:"ELIT Tokenizer"})," - a heuristic-based tokenizer"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>l});var s=t(6540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);