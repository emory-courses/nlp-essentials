"use strict";(globalThis.webpackChunknlp_essentials_textbook=globalThis.webpackChunknlp_essentials_textbook||[]).push([[823],{9250(e){e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"chaptersSidebar":[{"type":"category","label":"Getting Started","items":[{"type":"link","href":"/nlp-essentials/chapters/getting_started/overview","label":"Overview","docId":"chapters/getting_started/overview","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/getting_started/syllabus","label":"Syllabus","docId":"chapters/getting_started/syllabus","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/getting_started/schedule","label":"Schedule","docId":"chapters/getting_started/schedule","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/getting_started/development-environment","label":"Development Environment","docId":"chapters/getting_started/development-environment","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/getting_started/homework","label":"Homework","docId":"chapters/getting_started/homework","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Text Processing","items":[{"type":"link","href":"/nlp-essentials/chapters/text_processing/overview","label":"Overview","docId":"chapters/text_processing/overview","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/text_processing/frequency-analysis","label":"Frequency Analysis","docId":"chapters/text_processing/frequency-analysis","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/text_processing/tokenization","label":"Tokenization","docId":"chapters/text_processing/tokenization","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/text_processing/lemmatization","label":"Lemmatization","docId":"chapters/text_processing/lemmatization","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/text_processing/regular-expressions","label":"Regular Expressions","docId":"chapters/text_processing/regular-expressions","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/text_processing/homework","label":"Homework","docId":"chapters/text_processing/homework","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Language Models","items":[{"type":"link","href":"/nlp-essentials/chapters/language_models/overview","label":"Overview","docId":"chapters/language_models/overview","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/language_models/n-gram-models","label":"N-gram Models","docId":"chapters/language_models/n-gram-models","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/language_models/smoothing","label":"Smoothing","docId":"chapters/language_models/smoothing","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/language_models/maximum-likelihood-estimation","label":"Maximum Likelihood Estimation","docId":"chapters/language_models/maximum-likelihood-estimation","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/language_models/entropy-and-perplexity","label":"Entropy and Perplexity","docId":"chapters/language_models/entropy-and-perplexity","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/language_models/homework","label":"Homework","docId":"chapters/language_models/homework","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Large Language Models","items":[{"type":"link","href":"/nlp-essentials/chapters/large_language_models/overview","label":"Overview","docId":"chapters/large_language_models/overview","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/large_language_models/homework","label":"Homework","docId":"chapters/large_language_models/homework","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Vector Space Models","items":[{"type":"link","href":"/nlp-essentials/chapters/vector_space_models/overview","label":"Overview","docId":"chapters/vector_space_models/overview","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/vector_space_models/bag-of-words-model","label":"Bag-of-Words Model","docId":"chapters/vector_space_models/bag-of-words-model","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/vector_space_models/term-weighting","label":"Term Weighting","docId":"chapters/vector_space_models/term-weighting","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/vector_space_models/document-similarity","label":"Document Similarity","docId":"chapters/vector_space_models/document-similarity","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/vector_space_models/document-classification","label":"Document Classification","docId":"chapters/vector_space_models/document-classification","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/vector_space_models/homework","label":"Homework","docId":"chapters/vector_space_models/homework","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Distributional Semantics","items":[{"type":"link","href":"/nlp-essentials/chapters/distributional_semantics/overview","label":"Overview","docId":"chapters/distributional_semantics/overview","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/distributional_semantics/distributional-hypothesis","label":"Distributional Hypothesis","docId":"chapters/distributional_semantics/distributional-hypothesis","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/distributional_semantics/word-representations","label":"Word Representations","docId":"chapters/distributional_semantics/word-representations","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/distributional_semantics/latent-semantic-analysis","label":"Latent Semantic Analysis","docId":"chapters/distributional_semantics/latent-semantic-analysis","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/distributional_semantics/neural-networks","label":"Neural Networks","docId":"chapters/distributional_semantics/neural-networks","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/distributional_semantics/neural-language-models","label":"Neural Language Models","docId":"chapters/distributional_semantics/neural-language-models","unlisted":false},{"type":"link","href":"/nlp-essentials/chapters/distributional_semantics/homework","label":"Homework","docId":"chapters/distributional_semantics/homework","unlisted":false}],"collapsed":true,"collapsible":true}],"projectsSidebar":[{"type":"category","label":"Activities","items":[{"type":"link","href":"/nlp-essentials/projects/activities/overview","label":"Overview","docId":"projects/activities/overview","unlisted":false},{"type":"link","href":"/nlp-essentials/projects/activities/speed-dating","label":"Speed Dating","docId":"projects/activities/speed-dating","unlisted":false},{"type":"link","href":"/nlp-essentials/projects/activities/team-formation","label":"Team Formation","docId":"projects/activities/team-formation","unlisted":false},{"type":"link","href":"/nlp-essentials/projects/activities/project-pitch","label":"Project Pitch","docId":"projects/activities/project-pitch","unlisted":false},{"type":"link","href":"/nlp-essentials/projects/activities/proposal-report","label":"Proposal Report","docId":"projects/activities/proposal-report","unlisted":false},{"type":"link","href":"/nlp-essentials/projects/activities/live-demonstration","label":"Live Demonstration","docId":"projects/activities/live-demonstration","unlisted":false},{"type":"link","href":"/nlp-essentials/projects/activities/final-report","label":"Final Report","docId":"projects/activities/final-report","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Team Projects","items":[{"type":"link","href":"/nlp-essentials/projects/team_projects/current-projects","label":"Projects 2026","docId":"projects/team_projects/current-projects","unlisted":false},{"type":"link","href":"/nlp-essentials/projects/team_projects/projects-2025","label":"Projects 2025","docId":"projects/team_projects/projects-2025","unlisted":false},{"type":"link","href":"/nlp-essentials/projects/team_projects/projects-2024","label":"Projects 2024","docId":"projects/team_projects/projects-2024","unlisted":false},{"type":"link","href":"/nlp-essentials/projects/team_projects/projects-2023","label":"Projects 2023","docId":"projects/team_projects/projects-2023","unlisted":false},{"type":"link","href":"/nlp-essentials/projects/team_projects/projects-2022","label":"Projects 2022","docId":"projects/team_projects/projects-2022","unlisted":false},{"type":"link","href":"/nlp-essentials/projects/team_projects/projects-2021","label":"Projects 2021","docId":"projects/team_projects/projects-2021","unlisted":false},{"type":"link","href":"/nlp-essentials/projects/team_projects/projects-2020","label":"Projects 2020","docId":"projects/team_projects/projects-2020","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Project Ideas","items":[{"type":"link","href":"/nlp-essentials/projects/project_ideas/current-ideas","label":"Ideas 2026","docId":"projects/project_ideas/current-ideas","unlisted":false},{"type":"link","href":"/nlp-essentials/projects/project_ideas/ideas-2025","label":"Ideas 2025","docId":"projects/project_ideas/ideas-2025","unlisted":false},{"type":"link","href":"/nlp-essentials/projects/project_ideas/ideas-2024","label":"Ideas 2024","docId":"projects/project_ideas/ideas-2024","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"chapters/distributional_semantics/distributional-hypothesis":{"id":"chapters/distributional_semantics/distributional-hypothesis","title":"Distributional Hypothesis","description":"The distributional hypothesis suggests that words that occur in similar contexts tend to have similar meanings \\\\[1]. Let us examine the following two sentences with blanks:","sidebar":"chaptersSidebar"},"chapters/distributional_semantics/homework":{"id":"chapters/distributional_semantics/homework","title":"Homework","description":"HW4: Distributional Semantics","sidebar":"chaptersSidebar"},"chapters/distributional_semantics/latent-semantic-analysis":{"id":"chapters/distributional_semantics/latent-semantic-analysis","title":"Latent Semantic Analysis","description":"Latent Semantic Analysis (LSA) \\\\1] analyzes relationships between a set of documents and the terms they contain. It is based on the idea that words that are used in similar contexts tend to have similar meanings, which is in line with the [distributional hypothesis.","sidebar":"chaptersSidebar"},"chapters/distributional_semantics/neural-language-models":{"id":"chapters/distributional_semantics/neural-language-models","title":"Neural Language Models","description":"Neural language models leverage neural networks trained on extensive text data, enabling them to discern patterns and connections between terms and documents. Through this training, neural language models gain the ability to comprehend and generate human-like language with remarkable fluency and coherence.","sidebar":"chaptersSidebar"},"chapters/distributional_semantics/neural-networks":{"id":"chapters/distributional_semantics/neural-networks","title":"Neural Networks","description":"Logistic Regression","sidebar":"chaptersSidebar"},"chapters/distributional_semantics/overview":{"id":"chapters/distributional_semantics/overview","title":"Overview","description":"Distributional semantics represents the meaning of words based on their distributional properties in large corpora of text. It follows the distributional hypothesis, which states that \\"words with similar meanings tend to occur in similar contexts\\".","sidebar":"chaptersSidebar"},"chapters/distributional_semantics/word-representations":{"id":"chapters/distributional_semantics/word-representations","title":"Word Representations","description":"One-hot Encoding","sidebar":"chaptersSidebar"},"chapters/getting_started/development-environment":{"id":"chapters/getting_started/development-environment","title":"Development Environment","description":"This guide will help you set up your development environment by installing required tools: Python programming language, GitHub for version control, and PyCharm IDE.","sidebar":"chaptersSidebar"},"chapters/getting_started/homework":{"id":"chapters/getting_started/homework","title":"Homework","description":"HW0: Getting Started","sidebar":"chaptersSidebar"},"chapters/getting_started/overview":{"id":"chapters/getting_started/overview","title":"Overview","description":"This chapter introduces you to the course structure, learning objectives, and development environment setup. You will learn about course policies, grading criteria, and the tools you\'ll use throughout the semester. By the end of this chapter, you will have a fully configured development environment and be ready to start building natural language processing applications.","sidebar":"chaptersSidebar"},"chapters/getting_started/schedule":{"id":"chapters/getting_started/schedule","title":"Schedule","description":"CS|DATASCI|LING-329: Computational Linguistics (Spring 2026)","sidebar":"chaptersSidebar"},"chapters/getting_started/syllabus":{"id":"chapters/getting_started/syllabus","title":"Syllabus","description":"CS|DATASCI|LING-329: Computational Linguistics (Spring 2026)","sidebar":"chaptersSidebar"},"chapters/language_models/entropy-and-perplexity":{"id":"chapters/language_models/entropy-and-perplexity","title":"Entropy and Perplexity","description":"Entropy","sidebar":"chaptersSidebar"},"chapters/language_models/homework":{"id":"chapters/language_models/homework","title":"Homework","description":"HW2: Language Models","sidebar":"chaptersSidebar"},"chapters/language_models/maximum-likelihood-estimation":{"id":"chapters/language_models/maximum-likelihood-estimation","title":"Maximum Likelihood Estimation","description":"Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution based on observed data. MLE aims to find the values of the model\'s parameters that make the observed data most probable under the assumed statistical model.","sidebar":"chaptersSidebar"},"chapters/language_models/n-gram-models":{"id":"chapters/language_models/n-gram-models","title":"N-gram Models","description":"An n**-gram is a contiguous sequence of n items from text data. These items can be:","sidebar":"chaptersSidebar"},"chapters/language_models/overview":{"id":"chapters/language_models/overview","title":"Overview","description":"A language model is a computational model designed to understand, generate, and predict human language. It captures language patterns, learns the likelihood of a specific term occurring in a given context, and assigns probabilities to word sequences through training on extensive text data.","sidebar":"chaptersSidebar"},"chapters/language_models/smoothing":{"id":"chapters/language_models/smoothing","title":"Smoothing","description":"Unigram Smoothing","sidebar":"chaptersSidebar"},"chapters/large_language_models/homework":{"id":"chapters/large_language_models/homework","title":"Homework","description":"","sidebar":"chaptersSidebar"},"chapters/large_language_models/overview":{"id":"chapters/large_language_models/overview","title":"Overview","description":"Large language models (LLMs) have transformed how we interact with AI systems, enabling natural language conversations and complex reasoning tasks. Understanding how to effectively communicate with these models is fundamental to leveraging their capabilities and developing advanced AI systems.","sidebar":"chaptersSidebar"},"chapters/text_processing/frequency-analysis":{"id":"chapters/text_processing/frequency-analysis","title":"Frequency Analysis","description":"Frequency Analysis examines how often each word appears in a corpus. It helps understand language patterns and structure by measuring how often words appear in text.","sidebar":"chaptersSidebar"},"chapters/text_processing/homework":{"id":"chapters/text_processing/homework","title":"Homework","description":"HW1: Text Processing","sidebar":"chaptersSidebar"},"chapters/text_processing/lemmatization":{"id":"chapters/text_processing/lemmatization","title":"Lemmatization","description":"Sometimes, it is more appropriate to consider the canonical forms as tokens instead of their variations. For example, if you want to analyze the usage of the word \\"transformer\\" in NLP literature for each year, you want to count both \\"transformer\\" and \'transformers\' as a single item.","sidebar":"chaptersSidebar"},"chapters/text_processing/overview":{"id":"chapters/text_processing/overview","title":"Overview","description":"Text processing refers to the manipulation and analysis of textual data through techniques applied to raw text, making it more structured, understandable, and suitable for various applications.","sidebar":"chaptersSidebar"},"chapters/text_processing/regular-expressions":{"id":"chapters/text_processing/regular-expressions","title":"Regular Expressions","description":"Regular expressions, commonly abbreviated as regex, form a language for string matching, enabling operations to search, match, and manipulate text based on specific patterns or rules.","sidebar":"chaptersSidebar"},"chapters/text_processing/tokenization":{"id":"chapters/text_processing/tokenization","title":"Tokenization","description":"Tokenization is the process of breaking down a text into smaller units, typically words or subwords, known as tokens. Tokens serve as the basic building blocks used for a specific task.","sidebar":"chaptersSidebar"},"chapters/vector_space_models/bag-of-words-model":{"id":"chapters/vector_space_models/bag-of-words-model","title":"Bag-of-Words Model","description":"Overview","sidebar":"chaptersSidebar"},"chapters/vector_space_models/document-classification":{"id":"chapters/vector_space_models/document-classification","title":"Document Classification","description":"Document classification, also known as text classification, is a task that involves assigning predefined categories or labels to documents based on their content, used to automatically organize, categorize, or label large collections of textual documents.","sidebar":"chaptersSidebar"},"chapters/vector_space_models/document-similarity":{"id":"chapters/vector_space_models/document-similarity","title":"Document Similarity","description":"Let us vectorize the following three documents using the bag-of-words model with TF-IDF scores estimated from the chronicles\\\\of\\\\narnia.txt corpus:","sidebar":"chaptersSidebar"},"chapters/vector_space_models/homework":{"id":"chapters/vector_space_models/homework","title":"Homework","description":"HW3: Vector Space Models","sidebar":"chaptersSidebar"},"chapters/vector_space_models/overview":{"id":"chapters/vector_space_models/overview","title":"Overview","description":"A vector space model is a computational framework to represent text documents as vectors in a high-dimensional space such that each document is represented as a vector, and each dimension of the vector corresponds to a particular term in the vocabulary.","sidebar":"chaptersSidebar"},"chapters/vector_space_models/term-weighting":{"id":"chapters/vector_space_models/term-weighting","title":"Term Weighting","description":"Term Frequency","sidebar":"chaptersSidebar"},"intro":{"id":"intro","title":"NLP Essentials","description":"By Jinho D. Choi (2026 Edition)"},"projects/activities/final-report":{"id":"projects/activities/final-report","title":"Final Report","description":"Proposal","sidebar":"projectsSidebar"},"projects/activities/live-demonstration":{"id":"projects/activities/live-demonstration","title":"Live Demonstration","description":"Your team will participate in an interactive demonstration session where you will present your final project to multiple groups of classmates. The session will follow a rotation format, with each team hosting their station while other students move between presentations in 10-minute intervals.","sidebar":"projectsSidebar"},"projects/activities/overview":{"id":"projects/activities/overview","title":"Overview","description":"This section outlines the team project activities that span the second half of the semester. You will work collaboratively with your teammates to propose, develop, and demonstrate an innovative NLP application. By the end of this project sequence, you will have gained hands-on experience in planning, implementing, and presenting a real-world natural language processing solution.","sidebar":"projectsSidebar"},"projects/activities/project-pitch":{"id":"projects/activities/project-pitch","title":"Project Pitch","description":"Each team will deliver an 8-minute presentation pitching their project proposal to the class. This presentation should effectively communicate your project\'s vision, methodology, and potential impact while demonstrating its feasibility within the semester timeframe.","sidebar":"projectsSidebar"},"projects/activities/proposal-report":{"id":"projects/activities/proposal-report","title":"Proposal Report","description":"This half-semester-long project allows you to explore and contribute to the field of Natural Language Processing (NLP) through hands-on research and development. Working in teams, you will propose and execute an innovative NLP project that addresses a real-world problem.","sidebar":"projectsSidebar"},"projects/activities/speed-dating":{"id":"projects/activities/speed-dating","title":"Speed Dating","description":"During this class hour, you will have the opportunity to meet your classmates in a series of brief, focused conversations. The goal is to identify potential teammates whose research interests and skills complement your own. This is not about finding your exact clone, but rather discovering peers whose strengths and passions could contribute to a well-rounded and dynamic project team.","sidebar":"projectsSidebar"},"projects/activities/team-formation":{"id":"projects/activities/team-formation","title":"Team Formation","description":"Following our Project Ideas submissions and Speed Dating session, it is time to form your project teams. Each team will consist of 3-4 members.","sidebar":"projectsSidebar"},"projects/project_ideas/current-ideas":{"id":"projects/project_ideas/current-ideas","title":"Ideas 2026","description":"TBA","sidebar":"projectsSidebar"},"projects/project_ideas/ideas-2024":{"id":"projects/project_ideas/ideas-2024","title":"Ideas 2024","description":"Mara Adams","sidebar":"projectsSidebar"},"projects/project_ideas/ideas-2025":{"id":"projects/project_ideas/ideas-2025","title":"Ideas 2025","description":"Project Groups by Theme","sidebar":"projectsSidebar"},"projects/team_projects/current-projects":{"id":"projects/team_projects/current-projects","title":"Projects 2026","description":"TBA","sidebar":"projectsSidebar"},"projects/team_projects/projects-2020":{"id":"projects/team_projects/projects-2020","title":"Projects 2020","description":"[T1] Sabrina: Your Virtual Peer Mentor","sidebar":"projectsSidebar"},"projects/team_projects/projects-2021":{"id":"projects/team_projects/projects-2021","title":"Projects 2021","description":"[T1] Data Privacy: TEN Second TOS","sidebar":"projectsSidebar"},"projects/team_projects/projects-2022":{"id":"projects/team_projects/projects-2022","title":"Projects 2022","description":"[T1] Evaluation of Fake News Detection Models on COVID-19 Datasets","sidebar":"projectsSidebar"},"projects/team_projects/projects-2023":{"id":"projects/team_projects/projects-2023","title":"Projects 2023","description":"[T1] English Premier League Chatbot: Guide to the World\'s Most Exciting Football League","sidebar":"projectsSidebar"},"projects/team_projects/projects-2024":{"id":"projects/team_projects/projects-2024","title":"Projects 2024","description":"[T1] CAAP: Capture Assistant in Academic Papers","sidebar":"projectsSidebar"},"projects/team_projects/projects-2025":{"id":"projects/team_projects/projects-2025","title":"Projects 2025","description":"[T1] MedEase: AI-Powered Health Companion","sidebar":"projectsSidebar"}}}}')}}]);