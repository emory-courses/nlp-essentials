"use strict";(globalThis.webpackChunknlp_essentials_textbook=globalThis.webpackChunknlp_essentials_textbook||[]).push([[750],{5248(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"text_processing/lemmatization","title":"Lemmatization","description":"Sometimes, it is more appropriate to consider the canonical forms as tokens instead of their variations. For example, if you want to analyze the usage of the word \\"transformer\\" in NLP literature for each year, you want to count both \\"transformer\\" and \'transformers\' as a single item.","source":"@site/docs/text_processing/lemmatization.md","sourceDirName":"text_processing","slug":"/text_processing/lemmatization","permalink":"/nlp-essentials/text_processing/lemmatization","draft":false,"unlisted":false,"editUrl":"https://github.com/emory-courses/nlp-essentials/tree/main/docs/text_processing/lemmatization.md","tags":[],"version":"current","frontMatter":{"title":"Lemmatization"},"sidebar":"chaptersSidebar","previous":{"title":"Tokenization","permalink":"/nlp-essentials/text_processing/tokenization"},"next":{"title":"Regular Expressions","permalink":"/nlp-essentials/text_processing/regular-expressions"}}');var i=s(4848),t=s(8453);const o={title:"Lemmatization"},a=void 0,l={},c=[{value:"Lemma Lexica",id:"lemma-lexica",level:2},{value:"Lemmatizing",id:"lemmatizing",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:['Sometimes, it is more appropriate to consider the canonical forms as tokens instead of their variations. For example, if you want to analyze the usage of the word "',(0,i.jsx)(n.em,{children:"transformer"}),'" in NLP literature for each year, you want to count both "',(0,i.jsx)(n.em,{children:"transformer"}),"\" and '",(0,i.jsx)(n.em,{children:"transformers"}),"' as a single item."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Lemmatization"})," is a task that simplifies words into their base or dictionary forms, known as ",(0,i.jsx)(n.strong,{children:"lemmas"}),", to simplify the interpretation of their core meaning."]}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Q7"}),": What is the difference between a ",(0,i.jsx)(n.strong,{children:"lemmatizer"})," and a ",(0,i.jsx)(n.strong,{children:"stemmer"})," [3]?"]})}),"\n",(0,i.jsxs)(n.p,{children:["When analyzing ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/dat/word_types-token.txt",children:"dat/word_types-token.txt"})," obtained by the tokenizer in the previous section, the following tokens are recognized as separate word types:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Universities"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"University"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"universities"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"university"})}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:['Two variations are applied to the noun "',(0,i.jsx)(n.em,{children:"university"}),'" - ',(0,i.jsx)(n.strong,{children:"capitalization"}),", generally used for proper nouns or initial words, and ",(0,i.jsx)(n.strong,{children:"pluralization"}),", which indicates multiple instances of the term. On the other hand, verbs can also take several variations regarding ",(0,i.jsx)(n.strong,{children:"tense"})," and ",(0,i.jsx)(n.strong,{children:"aspect"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"study"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"studies"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"studied"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"studying"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"We want to develop a lemmatizer that normalizes all variations into their respective lemmas."}),"\n",(0,i.jsx)(n.h2,{id:"lemma-lexica",children:"Lemma Lexica"}),"\n",(0,i.jsx)(n.p,{children:"Let us create lexica for lemmatization:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:"import json\nimport os\nfrom types import SimpleNamespace\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L1: ",(0,i.jsx)(n.a,{href:"https://docs.python.org/3/library/json.html",children:"JSON encoder and decoder"})]}),"\n",(0,i.jsxs)(n.li,{children:["L2: ",(0,i.jsx)(n.a,{href:"https://docs.python.org/3/library/os.path.html#module-os.path",children:"Common pathname manipulations"})]}),"\n",(0,i.jsxs)(n.li,{children:["L3: ",(0,i.jsx)(n.a,{href:"https://docs.python.org/es/3.5/library/types.html#types.SimpleNamespace",children:"SimpleNamespace"})]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:"def get_lexica(res_dir: str) -> SimpleNamespace:\n    with open(os.path.join(res_dir, 'nouns.txt')) as fin: nouns = {noun.strip() for noun in fin}\n    with open(os.path.join(res_dir, 'verbs.txt')) as fin: verbs = {verb.strip() for verb in fin}\n    with open(os.path.join(res_dir, 'nouns_irregular.json')) as fin: nouns_irregular = json.load(fin)\n    with open(os.path.join(res_dir, 'verbs_irregular.json')) as fin: verbs_irregular = json.load(fin)\n    with open(os.path.join(res_dir, 'nouns_rules.json')) as fin: nouns_rules = json.load(fin)\n    with open(os.path.join(res_dir, 'verbs_rules.json')) as fin: verbs_rules = json.load(fin)\n\n    return SimpleNamespace(\n        nouns=nouns,\n        verbs=verbs,\n        nouns_irregular=nouns_irregular,\n        verbs_irregular=verbs_irregular,\n        nouns_rules=nouns_rules,\n        verbs_rules=verbs_rules\n    )\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L1: ",(0,i.jsx)(n.code,{children:"res_dir"}),": the path to the root directory where all lexica files are located."]}),"\n",(0,i.jsxs)(n.li,{children:["L2: ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/dat/nouns.txt",children:"nouns.txt"}),": a list of base nouns"]}),"\n",(0,i.jsxs)(n.li,{children:["L3: ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/dat/verbs.txt",children:"verbs.txt"}),": a list of base verbs"]}),"\n",(0,i.jsxs)(n.li,{children:["L4: ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/dat/nouns_irregular.json",children:"nouns_irregular.json"}),": a dictionary of nouns whose plural forms are irregular (e.g., ",(0,i.jsx)(n.em,{children:"mouse"})," \u2192 ",(0,i.jsx)(n.em,{children:"mice"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:["L5: ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/dat/verbs_irregular.json",children:"verbs_irregular.json"}),": a dictionary of verbs whose inflection forms are irregular (e.g., ",(0,i.jsx)(n.em,{children:"buy"})," \u2192 ",(0,i.jsx)(n.em,{children:"bought"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:["L6: ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/dat/nouns_rules.json",children:"nouns_rules.json"}),": a list of pluralization rules for nouns"]}),"\n",(0,i.jsxs)(n.li,{children:["L7: ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/dat/verbs_rules.json",children:"verbs_rules.json"}),": a list of inflection rules for verbs"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"We then verify that all lexical resources are loaded correctly:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'showLineNumbers title="Run"',children:"print(len(lexica.nouns))\nprint(len(lexica.verbs))\nprint(lexica.nouns_irregular)\nprint(lexica.verbs_irregular)\nprint(lexica.nouns_rules)\nprint(lexica.verbs_rules)\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",metastring:'title="Output"',children:"91\n27\n{'children': 'child', 'crises': 'crisis', 'mice': 'mouse'}\n{'is': 'be', 'was': 'be', 'has': 'have', 'had': 'have', 'bought': 'buy'}\n[['ies', 'y'], ['es', ''], ['s', ''], ['men', 'man'], ['ae', 'a'], ['i', 'us']]\n[['ies', 'y'], ['ied', 'y'], ['es', ''], ['ed', ''], ['s', ''], ['d', ''], ['ying', 'ie'], ['ing', ''], ['ing', 'e'], ['n', ''], ['ung', 'ing']]\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Q8"}),": What are the key differences between ",(0,i.jsx)(n.strong,{children:"inflectional"})," and ",(0,i.jsx)(n.strong,{children:"derivational"})," morphology?"]})}),"\n",(0,i.jsx)(n.h2,{id:"lemmatizing",children:"Lemmatizing"}),"\n",(0,i.jsxs)(n.p,{children:["Let us write the ",(0,i.jsx)(n.code,{children:"lemmatize()"})," function that takes a word and lemmatizes it using the lexica:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:"def lemmatize(lexica: SimpleNamespace, word: str) -> str:\n    def aux(word: str, vocabs: dict[str, str], irregular: dict[str, str], rules: list[tuple[str, str]]):\n        lemma = irregular.get(word, None)\n        if lemma is not None: return lemma\n\n        for p, s in rules:\n            lemma = word[:-len(p)] + s\n            if lemma in vocabs: return lemma\n\n        return None\n\n    word = word.lower()\n    lemma = aux(word, lexica.verbs, lexica.verbs_irregular, lexica.verbs_rules)\n\n    if lemma is None:\n        lemma = aux(word, lexica.nouns, lexica.nouns_irregular, lexica.nouns_rules)\n\n    return lemma if lemma else word\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L2: Define a nested function ",(0,i.jsx)(n.code,{children:"aux"})," to handle lemmatization."]}),"\n",(0,i.jsxs)(n.li,{children:["L3-4: Check if the word is in the irregular dictionary (",(0,i.jsx)(n.a,{href:"https://docs.python.org/3/library/stdtypes.html#dict.get",children:"get()"}),"), if so, return its lemma."]}),"\n",(0,i.jsxs)(n.li,{children:["L6-7: Try applying each rule in the ",(0,i.jsx)(n.code,{children:"rules"})," list to ",(0,i.jsx)(n.code,{children:"word"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"L8: If the resulting lemma is in the vocabulary, return it."}),"\n",(0,i.jsxs)(n.li,{children:["L10: If no lemma is found, return ",(0,i.jsx)(n.code,{children:"None"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"L12: Convert the input word to lowercase for case-insensitive processing."}),"\n",(0,i.jsx)(n.li,{children:"L13: Try to lemmatize the word using verb-related lexica."}),"\n",(0,i.jsx)(n.li,{children:"L15-16: If no lemma is found among verbs, try to lemmatize using noun-related lexica."}),"\n",(0,i.jsx)(n.li,{children:"L18: Return the lemma if found or the decapitalized word if no lemmatization occurred."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"We now test our lemmatizer for nouns and verbs:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'showLineNumbers title="Run"',children:"nouns = ['studies', 'crosses', 'areas', 'gentlemen', 'vertebrae', 'alumni', 'children', 'crises']\nnouns_lemmatized = [lemmatize(lexica, word) for word in nouns]\nfor word, lemma in zip(nouns, nouns_lemmatized): print('{} -> {}'.format(word, lemma))\n\nverbs = ['applies', 'cried', 'pushes', 'entered', 'takes', 'heard', 'lying', 'studying', 'taking', 'drawn', 'clung', 'was', 'bought']\nverbs_lemmatized = [lemmatize(lexica, word) for word in verbs]\nfor word, lemma in zip(verbs, verbs_lemmatized): print('{} -> {}'.format(word, lemma))\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",metastring:'title="Output: Nouns"',children:"studies -> study\ncrosses -> cross\nareas -> area\ngentlemen -> gentleman\nvertebrae -> vertebra\nalumni -> alumnus\nchildren -> child\ncrises -> crisis\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",metastring:'title="Output: Verbs"',children:"applies -> apply\ncried -> cry\npushes -> push\nentered -> enter\ntakes -> take\nheard -> hear\nlying -> lie\nstudying -> study\ntaking -> take\ndrawn -> draw\nclung -> cling\nwas -> be\nbought -> buy\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Finally, let us recount word types in ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/dat/emory-wiki.txt",children:"dat/emory-wiki.txt"})," using the lemmatizer and save them to ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/dat/word_types-token-lemma.txt",children:"dat/word_types-token-lemma.txt"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'showLineNumbers title="Run"',children:"from collections import Counter\nfrom src.tokenization import tokenize\n\ncorpus = 'dat/emory-wiki.txt'\ndelims = {'\"', \"'\", '(', ')', '[', ']', ':', '-', ',', '.'}\nwords = [lemmatize(lexica, word) for word in tokenize(corpus, delims)]\ncounts = Counter(words)\n\nprint(f'# of word tokens: {len(words)}')\nprint(f'# of word types: {len(counts)}')\n\noutput = 'dat/word_types-token-lemma.txt'\nwith open(output, 'w') as fout:\n    for key in sorted(counts.keys()): fout.write(f'{key}\\n')\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L2: Import the ",(0,i.jsx)(n.code,{children:"tokenize()"})," function from the ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/src/tokenization.py",children:"src/tokenization.py"})," module."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",metastring:'title="Output"',children:"# of word tokens: 363\n# of word types: 177\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/dat/text_processing/word_types-token-lemma.txt",children:"dat/text_processing /word_types-token-lemma.txt"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"When the words are further normalized by lemmatization, the number of word tokens remains the same as without lemmatization, but the number of word types is reduced from 197 to 177."}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Q9"}),": In which tasks can ",(0,i.jsx)(n.strong,{children:"lemmatization"})," negatively impact performance?"]})}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Source: ",(0,i.jsx)(n.a,{href:"https://github.com/emory-courses/nlp-essentials/blob/main/src/lemmatization.py",children:"lemmatization.py"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/emorynlp/elit-morph_analyzer",children:"ELIT Morphological Analyzer"})," - A heuristic-based lemmatizer"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://doi.org/10.1108/eb046814",children:"An Algorithm for Suffix Stripping"}),", Porter, Program: Electronic Library and Information Systems, 14(3), 1980 (",(0,i.jsx)(n.a,{href:"https://www.emerald.com/insight/content/doi/10.1108/00330330610681286/full/pdf",children:"PDF"}),")"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,s){s.d(n,{R:()=>o,x:()=>a});var r=s(6540);const i={},t=r.createContext(i);function o(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);